% !TeX spellcheck = en_GB
\documentclass[a4paper, 11pt]{article}
\usepackage[english]{babel}
\usepackage{newtxtext,newtxmath}

\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{dramatist}
\usepackage{dirtytalk}
\usepackage{multicol}
\usepackage{soul}
\usepackage{xcolor}

\usepackage[round]{natbib}

\usepackage{geometry}
\geometry{
	a4paper,
	total={150mm,257mm},
	top=20mm,
}

\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	filecolor=blue,      
	urlcolor=blue,
	citecolor=blue
}

\setlength\parindent{0pt}

%opening
\title{\textbf{Natural Language Processing}\\Portfolio II}
\author{\textbf{Leon F.A. Wetzel}\\ Information Science \\ Faculty of Arts - University of Groningen\\ \texttt{l.f.a.wetzel@student.rug.nl}}

\begin{document}

\maketitle

\begin{abstract}
	
	In this document, you can find the results and explanations for the assignments of the second part of the portfolio for the course Natural Language Processing, taught at the University of Groningen. The corresponding Python code can be found at \url{https://github.com/leonwetzel/natural-language-processing}\footnote{All code will be published after the course has been completed}. Note that version control of Jupyter notebooks is done via \texttt{jupytext}, so do not forget to convert the relevant Python scripts to notebooks yourself!

\end{abstract}

\section{Week 5 - Neural Language Models}

\subsection{Neurons}

\noindent\fbox{%
	\parbox{\textwidth}{%
Consider the basic neuron $y$ with a sigmoid activation $\sigma$ ($g$ in the image).

$$y = \sigma(x*w+b) \qquad \sigma = s(z) = \frac{1}{1 + e^{-z}} \qquad y = \sigma(x*w+b) = \frac{1}{1 + e^{-(x*w+b)}}$$

Given the weight and bias values $x = [0.3, 0.9, 0.1]$, $w = [-0.2, 0.8, -0.6]$, $b = 0.4$ compute the value of $y$ and provide intermediate calculations. 
	}%
}\\


\begin{equation} \label{eq1}
	\begin{split}
		y & = \frac{1}{1 + e^{-(w*x+b)}} = \frac{1}{1+e^{-(0.3*-0.2 + 0.9*0.8 + 0.1*-0.6 + 0.4)}} \\
		& = \frac{1}{1 + e^{-0.06 + 0.72 + -0.06 + 0.4}} = \frac{1}{1 + e^{-1}} \\
		& = \frac{1}{1 + 0.36787944117} = 0.73105857863
	\end{split}
\end{equation}

\subsection{Relu}

\noindent\fbox{%
	\parbox{\textwidth}{%
Assume the same basic neuron as in 5.1, but now with ReLU activation, where

$$\sigma = \text{relu}(z) = max(z,0)$$

Give the updated formula for computing $y$, as well as the new value of $y$ (with intermediate calculations).
	}%
}\\

$$y = \sigma(x*w+b) \qquad \sigma = \text{relu}(z) = max(z,\theta) \qquad y = \sigma(x*w+b) = max(x*w+b, 0)$$

\begin{equation} \label{eq1}
	\begin{split}
		y & = max(x*w+b, 0) = max(0.3*-0.2 + 0.9*0.8 + 0.1*-0.6 + 0.4, 0) \\
		& = max(-0.06 + 0.72 + -0.06 + 0.4, 0) \\
		& = max(1, 0) = 1
	\end{split}
\end{equation}

\subsection{Model size}

\noindent\fbox{%
	\parbox{\textwidth}{%
The feed-forward neural network below with a vocabulary of 50.000 words represented by one-hot vectors, a context window of 3 words, a projection layer with $d=100$, a hidden layer $d_h=500$, like the one below, has a total of $P = E + W + U$ trainable parameters. Which of the following has a bigger impact on the number of parameters:

\begin{enumerate}
	\item increasing context-size from 3 to 4, or
	\item increasing vocabulary from 50.000 to 51.000 words
\end{enumerate}
Give the calculations of $P$ to motivate your answer.
	}%
}\\

$$P(w_t = V_{42} | w_{t-3}, w_{t-2}, w_{t-3})$$

\subsection{Architectures}

\noindent\fbox{%
	\parbox{\textwidth}{%
Provide at least one example of an NLP application for each of the following task formulations. Try to come up with examples that are different from those mentioned in class.
	}%
}\\

\begin{table}[h]
	\centering
	\begin{tabular}{l|l}
	\textbf{Task formulation}	& \textbf{Example(s)} \\ \hline
	One-to-many	&  \\
	Many-to-one	& Language detection \\
	Many-to-many (seq2seq)	& Document summarization \\
	Many-to-many (sequence labelling) & Semantic tagging
	\end{tabular}
	\caption{Examples per task formulation for RNN's}
	\label{tab:examples_rnn}
\end{table}

\subsection{Probing}

\noindent\fbox{%
	\parbox{\textwidth}{%
Think of a grammatical phenomenon in a language of your choice, and come up with at least 10 example sentences to probe whether the model makes the correct predictions. Think of cases where the context makes it clear that the mask has to be plural or singular, that a verb has to have a particular form (like plural or singular, or participle or infinitive), that a specific (personal, possessive, reflexive) pronoun has to be used, that an adjective or noun has to have a specific inflection (like in German and more generally in languages with a rich case and/or gender marking system). There is a host of literature on this, see for instance Marvin and Linzen (for English) and Sahin et al (for multilingual probes).
	}%
}\\

asdsadasdsadsad\\

\noindent\fbox{%
	\parbox{\textwidth}{%
Give at least ten example sentences with a [MASK] and a list of targets that illustrate a specific grammatical phenomenon in a language of your choice. Describe what the grammatical phenomenon is you are investigating. Use the probe function for testing. Try to include both easy sentences (where the model should do well) as well as hard sentences (where there are words in the context that might lead to confusion, or where the clue words are far away from the mask). For languages other than Dutch or English, make sure to include enough explanation so that examples and tests are clear to a non-native speaker.

Describe how well the model did on your probe sentences. Where there any cases where the model made the wrong decision?
	}%
}\\

\end{document}
