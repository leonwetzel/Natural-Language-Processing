% !TeX spellcheck = en_GB
\documentclass[a4paper, 11pt]{article}
\usepackage[english]{babel}
\usepackage{newtxtext,newtxmath}

\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{dramatist}
\usepackage{dirtytalk}
\usepackage{multicol}
\usepackage{soul}
\usepackage{xcolor}

\usepackage[round]{natbib}

\usepackage{geometry}
\geometry{
	a4paper,
	total={150mm,257mm},
	top=20mm,
}

\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	filecolor=blue,      
	urlcolor=blue,
	citecolor=blue
}

\setlength\parindent{0pt}

%opening
\title{\textbf{Natural Language Processing}\\Portfolio II}
\author{\textbf{Leon F.A. Wetzel}\\ Information Science \\ Faculty of Arts - University of Groningen\\ \texttt{l.f.a.wetzel@student.rug.nl}}

\begin{document}

\maketitle

\begin{abstract}
	
	In this document, you can find the results and explanations for the assignments of the second part of the portfolio for the course Natural Language Processing, taught at the University of Groningen. The corresponding Python code can be found at \url{https://github.com/leonwetzel/natural-language-processing}\footnote{All code will be published after the course has been completed}. Note that version control of Jupyter notebooks is done via \texttt{jupytext}, so do not forget to convert the relevant Python scripts to notebooks yourself!

\end{abstract}

\section{Week 5 - Neural Language Models}

\subsection{Neurons}

\noindent\fbox{%
	\parbox{\textwidth}{%
Consider the basic neuron $y$ with a sigmoid activation $\sigma$ ($g$ in the image).

$$y = \sigma(x*w+b) \qquad \sigma = s(z) = \frac{1}{1 + e^{-z}} \qquad y = \sigma(x*w+b) = \frac{1}{1 + e^{-(x*w+b)}}$$

Given the weight and bias values $x = [0.3, 0.9, 0.1]$, $w = [-0.2, 0.8, -0.6]$, $b = 0.4$ compute the value of $y$ and provide intermediate calculations. 
	}%
}\\


\begin{equation} \label{eq1}
	\begin{split}
		y & = \frac{1}{1 + e^{-(w*x+b)}} = \frac{1}{1+e^{-(0.3*-0.2 + 0.9*0.8 + 0.1*-0.6 + 0.4)}} \\
		& = \frac{1}{1 + e^{-0.06 + 0.72 + -0.06 + 0.4}} = \frac{1}{1 + e^{-1}} \\
		& = \frac{1}{1 + 0.36787944117} = 0.73105857863
	\end{split}
\end{equation}

\subsection{Relu}

\noindent\fbox{%
	\parbox{\textwidth}{%
Assume the same basic neuron as in 5.1, but now with ReLU activation, where

$$\sigma = \text{relu}(z) = max(z,0)$$

Give the updated formula for computing $y$, as well as the new value of $y$ (with intermediate calculations).
	}%
}\\

$$y = \sigma(x*w+b) \qquad \sigma = \text{relu}(z) = max(z,\theta) \qquad y = \sigma(x*w+b) = max(x*w+b, 0)$$

\begin{equation} \label{eq1}
	\begin{split}
		y & = max(x*w+b, 0) = max(0.3*-0.2 + 0.9*0.8 + 0.1*-0.6 + 0.4, 0) \\
		& = max(-0.06 + 0.72 + -0.06 + 0.4, 0) \\
		& = max(1, 0) = 1
	\end{split}
\end{equation}

\subsection{Model size}

\noindent\fbox{%
	\parbox{\textwidth}{%
The feed-forward neural network below with a vocabulary of 50.000 words represented by one-hot vectors, a context window of 3 words, a projection layer with $d=100$, a hidden layer $d_h=500$, like the one below, has a total of $P = E + W + U$ trainable parameters. Which of the following has a bigger impact on the number of parameters:

\begin{enumerate}
	\item increasing context-size from 3 to 4, or
	\item increasing vocabulary from 50.000 to 51.000 words
\end{enumerate}
Give the calculations of $P$ to motivate your answer.
	}%
}\\

\begin{equation}
	\left.\begin{aligned}
		E &= d*|V| = 100 * 50.000 = 5.000.000\\
		W &= d_h * 3d = 500 * 3 * 100 = 150.000\\
		U &= |V| * d_h = 50.000 * 500 = 25.000.000\\
		P &= E + W + U = 5.000.000 + 150.000 + 25.000.000 = 30.150.000
	\end{aligned}
	\right\}
	\qquad \text{CS = 3}
\end{equation}

\begin{equation}
	\left.\begin{aligned}
		E &= d*|V| = 100 * 50.000 = 5.000.000\\
		W &= d_h * 4d = 500 * 4 * 100 = 200.000\\
		U &= |V| * d_h = 50.000 * 500 = 25.000.000\\
		P &= E + W + U = 5.000.000 + 200.000 + 25.000.000 = 30.200.000
	\end{aligned}
	\right\}
	\qquad \text{CS = 4}
\end{equation}

\begin{equation}
	\left.\begin{aligned}
		E &= d*|V| = 100 * 51.000 = 5.100.000\\
		W &= d_h * 3d = 500 * 3 * 100 = 150.000\\
		U &= |V| * d_h = 51.000 * 500 = 25.500.000\\
		P &= E + W + U = 5.100.000 + 150.000 + 25.500.000 = 30.750.000
	\end{aligned}
	\right\}
	\qquad \text{V = 51.000}
\end{equation}

Increasing the vocabulary size leads to a higher amount of trainable parameters, contrary to increasing the context-size. The size of vocabulary affects both $E$ and $U$ (opposed to only $W$ when changing the context size), which leads to higher values in the summation.
\subsection{Architectures}

\noindent\fbox{%
	\parbox{\textwidth}{%
Provide at least one example of an NLP application for each of the following task formulations. Try to come up with examples that are different from those mentioned in class.
	}%
}\\

\begin{table}[h]
	\centering
	\begin{tabular}{l|l}
	\textbf{Task formulation}	& \textbf{Example(s)} \\ \hline
	One-to-many	& Poem generation \\
	Many-to-one	& Language detection \\
	Many-to-many (seq2seq)	& Document summarization \\
	Many-to-many (sequence labelling) & Semantic tagging
	\end{tabular}
	\caption{Examples per task formulation for RNN's}
	\label{tab:examples_rnn}
\end{table}

\subsection{Probing}

\noindent\fbox{%
	\parbox{\textwidth}{%
Think of a grammatical phenomenon in a language of your choice, and come up with at least 10 example sentences to probe whether the model makes the correct predictions. Think of cases where the context makes it clear that the mask has to be plural or singular, that a verb has to have a particular form (like plural or singular, or participle or infinitive), that a specific (personal, possessive, reflexive) pronoun has to be used, that an adjective or noun has to have a specific inflection (like in German and more generally in languages with a rich case and/or gender marking system). There is a host of literature on this, see for instance Marvin and Linzen (for English) and Sahin et al (for multilingual probes).
	}%
}\\

\begin{enumerate}
	\item De minister-president heeft [MASK] handtekening gezet onder de bepaling.
		\begin{enumerate}
		\item de;
		\item een;
		\item geen;
		\item zijn;
		\item deze;
	\end{enumerate}
	\item De ontwikkelaars onthouden [MASK] van commentaar op hun code.
	\begin{enumerate}
		\item software;
		\item ook;
		\item gebruik;
		\item code;
		\item niet;
	\end{enumerate}
	\item Het ijsje begon met [MASK] toen de zon doorbrak.
		\begin{enumerate}
		\item ij;
		\item ,;
		\item zon;
		\item :;
		\item water;
	\end{enumerate}
	\item De universiteit investeerde [MASK] in onderzoeken naar kunstmatige intelligentie.
	\begin{enumerate}
		\item ook;
		\item verder;
		\item \#\#n;
		\item vooral;
		\item zich;
	\end{enumerate}
	\item Het zwaard van Damocles hing boven [MASK] hoofd.
	\begin{enumerate}
		\item zijn;
		\item het;
		\item de;
		\item haar;
		\item hun;
	\end{enumerate}
	\item De kaas uit Duitsland was niet [MASK] dan de kaas uit Nederland.
	\begin{enumerate}
		\item groter;
		\item meer;
		\item kleiner;
		\item anders;
		\item beter;
	\end{enumerate}
	\item De diplomaten dronken uit [MASK] bekers terwijl het ongeval plaatsvond.
	\begin{enumerate}
		\item de;
		\item in;
		\item het;
		\item uit;
		\item en;
	\end{enumerate}
	\item Het weer was onstuimig, het [MASK] namelijk vrij hard.
		\begin{enumerate}
		\item was;
		\item weer;
		\item is;
		\item kwam;
		\item had;
	\end{enumerate}
	\item De scholen sloten hun deuren [MASK].
		\begin{enumerate}
		\item in;
		\item aan;
		\item op;
		\item ';
		\item .;
	\end{enumerate}
	\item Jan had gisteren [MASK] voet gestoten tegen de tafelpoot.
		\begin{enumerate}
		\item en;
		\item met;
		\item van;
		\item aan;
		\item de;
	\end{enumerate}
\end{enumerate}

\noindent\fbox{%
	\parbox{\textwidth}{%
Give at least ten example sentences with a [MASK] and a list of targets that illustrate a specific grammatical phenomenon in a language of your choice. Describe what the grammatical phenomenon is you are investigating. Use the probe function for testing. Try to include both easy sentences (where the model should do well) as well as hard sentences (where there are words in the context that might lead to confusion, or where the clue words are far away from the mask). For languages other than Dutch or English, make sure to include enough explanation so that examples and tests are clear to a non-native speaker.

Describe how well the model did on your probe sentences. Where there any cases where the model made the wrong decision?
	}%
}\\

Please see the notebook for the sentences that were tested. The model did quite well on the provided probe sentences. There is a noticeable bias present in the model when it comes to gender and pronoun usage; him/his often comes on tops when such cases are masked. Although the model performs well in most cases, it did make a slight error in the sentence \texttt{The dog [MASK] hunting for food in the evenings.}, where \textit{are} apparently has a higher score than \textit{was}. 

\section{Week 6 - Dependency Parsing}

\subsection{Evaluation}

We are given the following sentence: \texttt{I hate to put a little pressure on you}\\

Our total amount of nodes ($N$) is 8, so our formulas would look like...

	\begin{equation}
		UAS = \frac{\text{\# of nodes with correct parent}}{8}
	\end{equation}

	\begin{equation}
		LAS = \frac{\text{\# of nodes with correct parent and edge label}}{8}
	\end{equation}

For the example on the upper side, the scores are as follows:

\noindent\begin{minipage}{.5\linewidth}
	\begin{equation}
		UAS = \frac{8}{8} = 1
	\end{equation}
\end{minipage}%
\begin{minipage}{.5\linewidth}
	\begin{equation}
		LAS = \frac{8}{8} = 1
	\end{equation}
\end{minipage}\\

For the example on the lower side, the scores are as follows:

\noindent\begin{minipage}{.5\linewidth}
	\begin{equation}
		UAS = \frac{6}{8} = 0.75
	\end{equation}
\end{minipage}%
\begin{minipage}{.5\linewidth}
	\begin{equation}
		LAS = \frac{3}{8} = 0.375
	\end{equation}
\end{minipage}

\subsection{Transition-based Parsing}

\noindent\fbox{%
	\parbox{\textwidth}{%
Describe the states and actions that a transition-based parser has to go through to produce the gold standard (top) analysis of the sentence
in question 6.1. A state consists of a stack, input buffer, and set of dependency relations. An action is either SHIFT, RIGHTARC, or LEFTARC,
where you can assume that the RIGHTARC and LEFTARC actions also produce the correctly labeled dependency relations.
	}%
}\\

We are given the following sentence: \texttt{I hate to put a little pressure on you}\\

jhhj

\subsection{Crossing arcs}

\noindent\fbox{%
	\parbox{\textwidth}{%
It is not possible to produce the analysis for the sentence below using a transition-based parser. Give the state and input buffer for the point where the problem arises and explain why there is no sequence of actions that leads to a correct parse.
	}%
}\\

We are given the following sentence: \texttt{Who did Kim talk to}\\

\subsection{Tiny-dependency parser}

\noindent\fbox{%
	\parbox{\textwidth}{%
Improve the oracle of the dependency parser in the notebook and train and evaluate it on the data. See notebook for details.
	}%
}\\

\end{document}
