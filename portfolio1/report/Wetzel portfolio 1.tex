% !TeX spellcheck = en_GB
\documentclass[a4paper, 11pt]{article}
\usepackage[english]{babel}
\usepackage{newtxtext,newtxmath}

\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{dramatist}
\usepackage{dirtytalk}

\usepackage[round]{natbib}

\usepackage{geometry}
\geometry{
	a4paper,
	total={150mm,257mm},
	top=20mm,
}

\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	filecolor=blue,      
	urlcolor=blue,
	citecolor=blue
}

\setlength\parindent{0pt}

%opening
\title{\textbf{Natural Language Processing}\\Portfolio I}
\author{\textbf{Leon F.A. Wetzel}\\ Information Science \\ University of Groningen - Faculty of Arts\\ \texttt{l.f.a.wetzel@student.rug.nl}}

\begin{document}

\maketitle

\begin{abstract}
	
	In this document, you can find the results and explanations for the assignments of the first part of the portfolio for the course Natural Language Processing, taught at the University of Groningen. The corresponding Python code can be found at \url{https://github.com/leonwetzel/natural-language-processing}\footnote{All code will be published after the course has been completed}. Note that version control of Jupyter notebooks is done via \texttt{jupytext}, so do not forget to convert the relevant Python scripts to notebooks yourself!

\end{abstract}

\section{Week 1 - Regular Expressions}

We used \url{https://regex101.com/} to test our regular expressions. In this document, we use \texttt{$\wedge$} to better display/represent a caret. Spaces cannot be displayed in \LaTeX  sadly, so these are not shown below.

\subsection{Regular Expressions I}

\begin{enumerate}
	\item Set of all alphabetic strings. \\ $ \wedge [a-zA-z ]+\$ $
	
	\item Set of all lower case alphabetic strings ending in $b$. \\ $\wedge [a-z ]*[b]\$ $
	
	\item Set of all strings from the alphabet ${a,b}$ such that each $a$ is immediately preceded by a $b$ and immediately followed by a $b$. \\$\wedge(bab)+\$ $
\end{enumerate}

\subsection{Regular Expressions II}

\begin{enumerate}
	\item Set of all strings with two consecutive repeated words (e.g., \textit{“Humbert Humbert”} and \textit{“the the”} but not \textit{“the bug”} or \textit{“the big and the bug”}). \\ $ \textbackslash b(\textbackslash w+)\textbackslash b \textbackslash s+ \textbackslash1 $
	
	\item All strings that start at the beginning of the line with an integer and that end with a word. \\ $ \wedge \textbackslash d+.+ \textbackslash b[a-zA-Z]+\textbackslash b \$ $
	
	\item All strings that have both the word \textit{grotto} and the word \textit{raven} in them (but not, e.g., words like \textit{grottos} that merely contain the word \textit{grotto}). \\ $ \textbackslash b \space { grotto} \textbackslash b.* | \textbackslash b \space { raven} \textbackslash b.*  $
\end{enumerate}

\subsection{ELIZA}

We added extra regular expressions and substitutions to handle more variants of user input. We also implemented a substitution to handle question marks, so that ELIZA's responses feel more like proper questions when relevant. The other implemented substitutions are related to how one feels, and takes into account words such as \textit{encounter}, \textit{experience} and \textit{suffer from}. Snippets from example dialogue can be found below.

\begin{drama}
	\Character{ELIZA}{eliza}
	\Character{Leon}{leon}
	
	\elizaspeaks: How are you?  
	
	\leonspeaks: I experience coldness.
	
	\elizaspeaks: Why do you experience coldness?
	
	\leonspeaks: I feel as if it is snowing today
	
	\elizaspeaks: Why do you feel as if it is snowing today?
	
	\leonspeaks: Because it is snowing today
	
	\elizaspeaks: Because it is snowing today?
	
	\leonspeaks: Yes, but I am inside
	
	\elizaspeaks: Yes, but Why are you inside?
	
	\leonspeaks: Because it is cold outside
	
	\elizaspeaks: Because it is cold outside?
	
	\leonspeaks: Yes
	
\end{drama}

\subsection{Byte-Pair Encoding}

Our example text for the experiment is a snippet from the Wikipedia page on the GameStop short squeeze\footnote{\url{https://en.wikipedia.org/wiki/GameStop_short_squeeze}}, which occurred in January 2021. According to \url{https://wordcounter.net}, our example consists of 137 words and 827 characters.\\

"\textit{In January 2021, a short squeeze of the stock of the American video game retailer GameStop (NYSE: GME) and other securities took place, causing major financial consequences for certain hedge funds and large losses for short sellers. Approximately 140 percent of GameStop's public float had been sold short, and the rush to buy shares to cover those positions as the price rose caused it to rise even further. The short squeeze was initially and primarily triggered by users of the subreddit r/wallstreetbets, an Internet forum on the social news website Reddit. At its height, on January 28, the short squeeze caused the retailer's stock price to reach a pre-market value of over US\$500 per share, nearly 30 times the \$17.25 valuation at the beginning of the month. The price of many other heavily shorted securities increased.}"\\

The default value for \texttt{vocab\_size} ($v$) is $30000$. With this value, we can count 164 words and 190 segments found in our example text by the tokenizer. For $v=25000$, we count 164 words and 195 segments, and the counts for $v=20000$ are 164 words and 197 segments. In line with the assignment description, we can observe here that more words are segmented into subwords when the number of segments increase (as a result of $v$ decreasing). An overview of the results can be found in table \ref{tab:tokenizer_results}.

\begin{table}[h]
	\centering
	\begin{tabular}{l|l|l|l|l|l|l|l|l}
	$v$		 & 30000 & 25000 & 20000  & 15000 & 10000 & 7500 & 5000 & 2500\\ \hline
	Words 	 & 164 	 & 164   & 164    & 164   & 164   & 164  & 164  & 164\\ 
	Segments & 190   & 195   & 197    & 204	  & 222   & 246  & 692  & 692
	\end{tabular}
	\caption{Overview of \texttt{vocab\_size} values and the word and segment counts}
	\label{tab:tokenizer_results}
\end{table}

When $v=7500$, the number of segments is approximately 150\% of the number of words. The longest words in our example text during this setup that were not segmented, are \textit{increased}, \textit{beginning} and \textit{initially}. All these words consist of nine characters each.

\section{Week 2 - N-gram Language Models}

For this week's assignment, we take a look at several assignments from \citet{jurafskyspeech}.

\subsection{J\&M exercise 3.1}

\noindent\fbox{%
	\parbox{\textwidth}{%
Write out the equation for trigram probability estimation (modifying Eq. 3.11). Now write out all the non-zero trigram probabilities for the I am Sam corpus on page 41.
	}%
}\\


The original - simplified - formula for bigram probability estimation is as follows:

$$P(w_{n} | w_{n-1}) = \frac{C(w_{n-1}w_{n})}{C(w_{n-1})}$$

\subsection{J\&M exercise 3.2}

\noindent\fbox{%
	\parbox{\textwidth}{%
Calculate the probability of the sentence \textit{i want chinese food}. Give two probabilities, one using Fig. 3.2, and another using the add-1 smoothed table in Fig. 3.6.
	}%
}\\

ahdhkshjhdishdius

\subsection{J\&M exercise 3.6}

\noindent\fbox{%
	\parbox{\textwidth}{%
Suppose we train a trigram language model with add-one smoothing on a given corpus. The corpus contains $V$ word types. Express a formula for estimating $P(w3|w1,w2)$, where $w3$ is a word which follows the bigram $(w1,w2)$, in terms of various N-gram counts and $V$. Use the notation $c(w1,w2,w3)$ to denote the number of times that trigram $(w1,w2,w3)$ occurs in the corpus, and so on for bigrams and unigrams.
	}%
}\\

sadsadsadsakdjsalkdj

\subsection{J\&M exercise 3.7}

\noindent\fbox{%
	\parbox{\textwidth}{%
We are given the following corpus, modified from the one in the chapter:\\
<s> I am Sam </s>\\
<s> Sam I am </s>\\
<s> I am Sam </s>\\
<s> I do not  like green eggs and Sam </s>\\
If we use linear interpolation smoothing between a maximum-likelihood bi-gram model and a maximum-likelihood unigram model with $\lambda1 = 1/2$ and $\lambda2 = 1/2$ , what is $P(Sam|am)$? Include <s> and </s> in your counts just like any other token.
	}%
}\\


sadsaldjsakdjlsadjsadsad

\subsection{N-grams in the notebook}

\noindent\fbox{%
	\parbox{\textwidth}{%
What are the different and common n-grams? You may ignore n-grams containing punctuation symbols.
	}%
}\\

The X are different N-grams, whereas the Y are common N-grams.\\

\noindent\fbox{%
	\parbox{\textwidth}{%
Describe in some detail (but in at most approximately 100 words) how one can implement a generation program based on a trigram language model.
	}%
}\\

asdsadsad sadsadsadsa

\bibliographystyle{plainnat}
\bibliography{refs}

\end{document}
