{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-gram statistics \n",
    "\n",
    "Make sure to install the [nltk library](http://www.nltk.org/install.html)  and download the [Corpus of Presidential Speeches](http://www.thegrammarlab.com/?nor-portfolio=corpus-of-presidential-speeches-cops-and-a-clintontrump-corpus)  for experiments.\n",
    "\n",
    "Use the script below to compare (manually or by adding code) the 25  most frequent **bigrams** and **trigrams** for two presidents of your choice. \n",
    "\n",
    "What are the different and common n-grams? You may ignore n-grams containing punctuation symbols.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(',', 'and') 492\n",
      "('of', 'the') 319\n",
      "('in', 'the') 241\n",
      "('.', 'he') 199\n",
      "('’', 's') 186\n",
      "('.', 'the') 184\n",
      "('to', 'the') 154\n",
      "(',', 'the') 148\n",
      "('he', 'was') 140\n",
      "('it', 'was') 125\n",
      "('and', 'the') 123\n",
      "(',', 'he') 119\n",
      "('project', 'gutenberg-tm') 96\n",
      "('on', 'the') 94\n",
      "(',', 'but') 93\n",
      "('with', 'the') 88\n",
      "('.', 'it') 84\n",
      "('he', 'had') 84\n",
      "('.', 'they') 79\n",
      "('at', 'the') 79\n",
      "('for', 'the') 75\n",
      "('into', 'the') 74\n",
      "('from', 'the') 72\n",
      "('by', 'the') 71\n",
      "('and', 'he') 70\n",
      "('in', 'a') 68\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams, word_tokenize\n",
    "from nltk.text import Text\n",
    "from collections import Counter\n",
    "import fileinput, glob \n",
    "\n",
    "n = 2   # ngram size\n",
    "\n",
    "corpus = glob.glob('*.txt')   # text to analyze, replace with path to one of the corpus subdirectories \n",
    "allngrams = []\n",
    "for sentence in fileinput.input(files=corpus):\n",
    "        # lower-case input, do basic tokenization, create ngrams of length n\n",
    "        sentencegrams = ngrams(word_tokenize(sentence.lower()),n)\n",
    "        words = word_tokenize(sentence.lower())\n",
    "        allngrams.extend(list(sentencegrams))\n",
    "        \n",
    "# print most frequent ngrams sorted by frequency\n",
    "nbest = 25 # modify if you want to see more results \n",
    "for (key,val) in Counter(allngrams).most_common() :\n",
    "    if (nbest >= 0) :\n",
    "            print(key,val)\n",
    "    nbest -= 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-gram generation \n",
    "\n",
    "You can run the code below to generate a random text based on a trigram language model trained on the files specified in **corpus**. \n",
    "\n",
    "1. Run the code to generate an example text \n",
    "\n",
    "2. Describe in some detail (but in at most approximately 100 words) how one can implement a generation program based on a trigram language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building ngram index...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one who was now limping in earnest . them booming up the bank and he\n",
      "knew not where or why , the judge ’ s warning to take de job . this or\n",
      "any files containing a part of the latest dynasty , a snapping , the\n",
      "calves he had started on toward the worst part of this work to protect\n",
      "the project gutenberg literary archive foundation . on the head of the\n",
      "country where you are located also govern what you can do with most\n",
      "project gutenberg-tm electronic work under this agreement shall be\n",
      "interpreted to make donations to\n"
     ]
    }
   ],
   "source": [
    "import random, fileinput, glob \n",
    "\n",
    "corpus = glob.glob('*.txt') # replace with the path to directory with text files \n",
    "\n",
    "seed = random.randint(0,1000)\n",
    "allwords = []\n",
    "# read input from files given as command-line argument\n",
    "for sentence in fileinput.input(files=corpus):\n",
    "        # lower-case input, do basic tokenization,\n",
    "        words = word_tokenize(sentence.lower())\n",
    "        allwords.extend(words)\n",
    "\n",
    "alltext = Text(allwords)\n",
    "generated = alltext.generate(random_seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
